{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"\n# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATASETS\n# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n# THEN FEEL FREE TO DELETE CELL.\n\nimport os\nimport sys\nfrom tempfile import NamedTemporaryFile\nfrom urllib.request import urlopen\nfrom urllib.parse import unquote\nfrom urllib.error import HTTPError\nfrom zipfile import ZipFile\n\nCHUNK_SIZE = 40960\nDATASET_MAPPING = 'cat-and-dog:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F23777%2F30378%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20211008%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20211008T004639Z%26X-Goog-Expires%3D259199%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D88e222215087d2953b24e12149810a14aefab660be34ca5a926c456fa8960f868b22ad69903b5706d8e95b85ac4cae6bfccfd791581d20b0a8ee0ec099ebf17d44e0a920aa773d8b14812ffb36710a73916c4381aa057a3c22188c1fcb8f220b03d9f398817cbd770fd7a24a90768bf932d104ecf0492c8e6cd8b6436fd0efe43062034bcc471048cc3e0f504080e640e4e6cdeef5da63d4f1561d40e5111441fe8473e5459e625cab681d2f15f13d4fa27b122841d8e946ad5bae734ac6ad41ed8dfd45c6ae1a165ca04b01efbc3c901e020c4d510d0f4905c7ab63f0568d99508dc9cef40f2eef4b00ee024018b65c61cd4a08ae80bd60b8bdbaacfbce4e2e'\nKAGGLE_INPUT_PATH='/home/kaggle/input'\nKAGGLE_INPUT_SYMLINK='/kaggle'\n\nos.makedirs(KAGGLE_INPUT_PATH, 777)\nos.symlink(KAGGLE_INPUT_PATH, os.path.join('..', 'input'), target_is_directory=True)\nos.makedirs(KAGGLE_INPUT_SYMLINK)\nos.symlink(KAGGLE_INPUT_PATH, os.path.join(KAGGLE_INPUT_SYMLINK, 'input'), target_is_directory=True)\n\nfor dataset_mapping in DATASET_MAPPING.split(','):\n    directory, download_url_encoded = dataset_mapping.split(':')\n    download_url = unquote(download_url_encoded)\n    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n    try:\n        with urlopen(download_url) as zipfileres, NamedTemporaryFile() as tfile:\n            total_length = zipfileres.headers['content-length']\n            print(f'Downloading {directory}, {total_length} bytes zipped')\n            dl = 0\n            data = zipfileres.read(CHUNK_SIZE)\n            while len(data) > 0:\n                dl += len(data)\n                tfile.write(data)\n                done = int(50 * dl / int(total_length))\n                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n                sys.stdout.flush()\n                data = zipfileres.read(CHUNK_SIZE)\n            print(f'\\nUnzipping {directory}')\n            with ZipFile(tfile) as zfile:\n                zfile.extractall(destination_path)\n    except HTTPError as e:\n        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n        continue\n    except OSError as e:\n        print(f'Failed to load {download_url} to path {destination_path}')\n        continue\nprint('Dataset import complete.')\n","metadata":{},"cell_type":"code","outputs":[],"execution_count":0},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os, shutil\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n        \n    \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-07T23:29:58.049116Z","iopub.execute_input":"2021-10-07T23:29:58.049648Z","iopub.status.idle":"2021-10-07T23:30:03.983216Z","shell.execute_reply.started":"2021-10-07T23:29:58.049529Z","shell.execute_reply":"2021-10-07T23:30:03.982295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_dataset_dir = '/kaggle/input/cat-and-dog/'\nbase_dir = '/kaggle/cat-and-dog_small/'\n\nos.mkdir(base_dir)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T23:30:03.984754Z","iopub.execute_input":"2021-10-07T23:30:03.985011Z","iopub.status.idle":"2021-10-07T23:30:03.989912Z","shell.execute_reply.started":"2021-10-07T23:30:03.984984Z","shell.execute_reply":"2021-10-07T23:30:03.988986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = os.path.join(base_dir, 'train')\nos.mkdir(train_dir)\nvalidation_dir = os.path.join(base_dir, 'validation')\nos.mkdir(validation_dir)\ntest_dir = os.path.join(base_dir, 'test')\nos.mkdir(test_dir)\ntrain_cats_dir = os.path.join(train_dir, 'cats')\nos.mkdir(train_cats_dir)\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\nos.mkdir(train_dogs_dir)\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')\nos.mkdir(validation_cats_dir)\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')\nos.mkdir(validation_dogs_dir)\ntest_cats_dir = os.path.join(test_dir, 'cats')\nos.mkdir(test_cats_dir)\ntest_dogs_dir = os.path.join(test_dir, 'dogs')\nos.mkdir(test_dogs_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating cat dataset\nfnames = ['cat.{}.jpg'.format(i) for i in range(1, 1001)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir+'training_set/training_set/cats/', fname)\n    dst = os.path.join(train_cats_dir, fname)\n    shutil.copyfile(src, dst)\nfnames = ['cat.{}.jpg'.format(i) for i in range(1001, 1501)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir+'training_set/training_set/cats/', fname)\n    dst = os.path.join(validation_cats_dir, fname)\n    shutil.copyfile(src, dst)\nfnames = ['cat.{}.jpg'.format(i) for i in range(4001, 4501)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir+'test_set/test_set/cats/', fname)\n    dst = os.path.join(test_cats_dir, fname)\n    shutil.copyfile(src, dst)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Validating the images in each cat dataset\nprint('total training cat images:', len(os.listdir(train_cats_dir)))\nprint('total validation cat images:', len(os.listdir(validation_cats_dir)))\nprint('total test cat images:', len(os.listdir(test_cats_dir)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating dog dataset\nfnames = ['dog.{}.jpg'.format(i) for i in range(1, 1001)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir+'training_set/training_set/dogs/', fname)\n    dst = os.path.join(train_dogs_dir, fname)\n    shutil.copyfile(src, dst)\nfnames = ['dog.{}.jpg'.format(i) for i in range(1001, 1501)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir+'training_set/training_set/dogs/', fname)\n    dst = os.path.join(validation_dogs_dir, fname)\n    shutil.copyfile(src, dst)\nfnames = ['dog.{}.jpg'.format(i) for i in range(4001, 4501)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir+'test_set/test_set/dogs/', fname)\n    dst = os.path.join(test_dogs_dir, fname)\n    shutil.copyfile(src, dst)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Validating the images in each dog dataset\nprint('total training dog images:', len(os.listdir(train_dogs_dir)))\nprint('total validation dog images:', len(os.listdir(validation_dogs_dir)))\nprint('total test dog images:', len(os.listdir(test_dogs_dir)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating a convet model\nmodel = keras.Sequential(([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(), \n    layers.Dense(512, activation = 'relu'), \n    layers.Dense(1, activation = 'sigmoid')\n]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\noptimizer=optimizers.RMSprop(lr=1e-4),\nmetrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data prepocessing\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nvalidation_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(150, 150),\n    batch_size=20,\n    class_mode='binary')\nvalidation_generator = validation_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(150, 150),\n    batch_size=20,\n    class_mode='binary')\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(150, 150),\n    class_mode='binary')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit_generator(\n    train_generator,\n    steps_per_epoch=100,\n    epochs=5,\n    validation_data=validation_generator,\n    validation_steps=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history.history.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}